{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41561d62-704b-4ec9-b2b7-439a01cfb793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from maptd_model import maptd_model\n",
    "from IPython import display\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from tkinter.filedialog import askdirectory\n",
    "from pipeline_v2 import get_dataset, get_dataset_from_txt_files\n",
    "from losses import total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b58649-c3a9-4038-aae0-305aaaf49c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffcc72-77fc-4ba9-828b-06b47c0c009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=[128, 128, 3], name='input_image')\n",
    "    tar_score = tf.keras.layers.Input(shape=[128, 128, 1], name='target_score_map')\n",
    "    tar_geo = tf.keras.layers.Input(shape=[128, 128, 5], name='target_geo_map')\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar_score, tar_geo])  # (batch_size, 512, 512, 9)\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x)  # (batch_size, 256, 256, 64)\n",
    "    down2 = downsample(128, 4)(down1)  # (batch_size, 128, 128, 128)\n",
    "    down3 = downsample(256, 4)(down2)  # (batch_size, 64, 64, 256)\n",
    "    down4 = downsample(512, 4)(down3) # (batch_size, 32, 32, 512) #NOTE: Gerasimos changed the code here, it is not exactly the same as in pix2pix\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down4)  # (batch_size, 34, 34, 256)\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar_score, tar_geo], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009029ad-93b1-49f3-9488-747fd5012bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 100\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(disc_generated_output, score_map, gen_score_map, geo_map, \n",
    "                   gen_geo_map, training_mask):\n",
    "    \n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    loss = total_loss(score_map, gen_score_map, geo_map, gen_geo_map, training_mask)\n",
    "\n",
    "    total_gen_loss = gan_loss + LAMBDA * loss\n",
    "\n",
    "    return total_gen_loss, gan_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc3752-b1d3-4ebe-913a-53fbbc17526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdffd11-651c-41e3-8bfd-5718a9d4ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_score_map(tile, gt_score_map, pred_score_map, threshold=None):\n",
    "    pred_score_map = np.where(pred_score_map > threshold, 1, 0)\n",
    "    COLORMAP = 'gray'\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
    "    ax[0].imshow(np.squeeze(tf.cast(tile, tf.uint8)))\n",
    "    ax[1].imshow(np.squeeze(gt_score_map), cmap=COLORMAP)\n",
    "    ax[2].imshow(np.squeeze(pred_score_map), cmap=COLORMAP)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9de2ba-a9a0-44ee-8b76-97256a03564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepDecayLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, decay_rate, decay_every_n_steps):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_every_n_steps = decay_every_n_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        exp = tf.floor(step/self.decay_every_n_steps)\n",
    "        return self.initial_learning_rate * tf.pow(self.decay_rate, exp)\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, decay_rate, decay_on_step):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_on_step = decay_on_step\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        new_lr = tf.cond(step < self.decay_on_step, \n",
    "                       lambda: self.initial_learning_rate,\n",
    "                       lambda: self.initial_learning_rate * self.decay_rate)\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1563854-dc2c-46f0-ae61-eceef6837cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'D:/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/general_dataset/'\n",
    "INITIAL_DIR = 'D:/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/ckpts/models'\n",
    "\n",
    "conf_file_dir = askdirectory(initialdir=INITIAL_DIR, title='Select the directory '\n",
    "    'with the configuration files')\n",
    "print(conf_file_dir)\n",
    "\n",
    "# Create log files\n",
    "log_dir = os.path.join(conf_file_dir, 'gan_logs')\n",
    "sum_writer = tf.summary.create_file_writer(os.path.join(\n",
    "    log_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977dde8-a6f3-4266-8304-a5f9d0f7d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "generator = maptd_model()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Define the optimizers\n",
    "step_to_reduce_lr=2**17\n",
    "initial_lr = 1e-4\n",
    "decay_rate = 0.1\n",
    "generator_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=StepDecayLRSchedule(initial_lr, \n",
    "                                      decay_rate,\n",
    "                                      step_to_reduce_lr), epsilon=1e-8)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "\n",
    "# Initialize checkpoints\n",
    "ckpt_dir = os.path.join(conf_file_dir, 'gan_training_ckpts')\n",
    "ckpt_prefix = os.path.join(ckpt_dir, 'ckpt')\n",
    "ckpt = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                           discriminator_optimizer=discriminator_optimizer,\n",
    "                           generator=generator,\n",
    "                           discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e516745-a2d3-4a2d-8c50-87316cff2664",
   "metadata": {},
   "source": [
    "#### Run the commands in the following cell if you want to continue from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ce21e-33f6-4f7b-85a9-edf870f75f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "print(latest)\n",
    "ckpt.restore(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd3f3f-16f5-48e9-890b-6c945ee607fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(generator, discriminator, tile, \n",
    "               score_map, geo_map, training_mask, \n",
    "               step, summary_writer):\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        \n",
    "        gen_score_map, gen_geo_map = generator(tile, training=True)\n",
    "        \n",
    "        resized_input_image = tile[:, ::4, ::4, :]\n",
    "        \n",
    "        disc_real_output = discriminator([resized_input_image, score_map, \n",
    "                                          geo_map], training=True)\n",
    "        \n",
    "        disc_generated_output = discriminator([resized_input_image, \n",
    "                                               gen_score_map, gen_geo_map], \n",
    "                                              training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_loss = generator_loss(\n",
    "            disc_generated_output, score_map, gen_score_map, geo_map, \n",
    "            gen_geo_map, training_mask)\n",
    "        \n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_loss', gen_loss, step=step//1000)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b442fbc-7259-46a3-b4af-24a19f9572da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(generator, discriminator, train_dataset, test_dataset, summary_writer,\n",
    "        total_steps=2**20, enumerate_from=0):\n",
    "   \n",
    "    training_start = time.time()\n",
    "    start = training_start\n",
    "    for step, (tile, score_map, geo_map, training_mask) in \\\n",
    "                train_dataset.repeat().take(total_steps - enumerate_from).\\\n",
    "                enumerate(start=enumerate_from):\n",
    "        \n",
    "        train_step(generator, discriminator, tile,\n",
    "               score_map, geo_map, training_mask, \n",
    "               step, summary_writer)\n",
    "        \n",
    "        step = step.numpy()\n",
    "        \n",
    "        if (step + 1) % 500 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            print(datetime.datetime.now().strftime(\"%H:%M:%S\"))            \n",
    "            print(f'Step {step + 1}/{total_steps}')\n",
    "            \n",
    "            if step != 0 and step != enumerate_from:\n",
    "                print(f'Time taken for the last 500 steps: '\n",
    "                      f'{time.time()-start:.2f} sec')\n",
    "                estimated_remaining_time = int((time.time() - training_start) / \\\n",
    "                    (step - enumerate_from) * (total_steps - step))\n",
    "                print(f'Estimated time for the training to finish: '\n",
    "                      f'{estimated_remaining_time // 3600} hrs, '\n",
    "                      f'{int(estimated_remaining_time % 3600 / 60)} mins')\n",
    "            \n",
    "            print('Current Learning Rate: ', generator_optimizer.lr(step))\n",
    "            \n",
    "            example_tile, example_score_map, _, _ = next(iter(test_dataset.take(1)))\n",
    "            example_pred_score_map, _ = generator(example_tile, training=True)\n",
    "            show_score_map(example_tile, \n",
    "                           example_score_map, \n",
    "                           example_pred_score_map, \n",
    "                           threshold=0.8)    \n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "        if (step + 1) % 10000 == 0:\n",
    "            ckpt.save(file_prefix=ckpt_prefix)\n",
    "            print(f'Checkpoint at step: {step + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f1ab9-3241-4077-aa41-b3fbd5e05314",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 8\n",
    "# Load the dataset generators\n",
    "train_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'train', \n",
    "                                      batch_size=TRAIN_BATCH_SIZE)\n",
    "test_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'test', \n",
    "                                     batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811dd45-1e7d-4b29-aa56-4752e9125358",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(generator, discriminator, train_ds, test_ds, sum_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086cea36-c83a-491b-87f7-feedd9a52944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepenv",
   "language": "python",
   "name": "deepenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
