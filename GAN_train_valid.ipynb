{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41561d62-704b-4ec9-b2b7-439a01cfb793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from maptd_model import maptd_model\n",
    "from IPython import display\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import stats\n",
    "\n",
    "from tkinter.filedialog import askdirectory\n",
    "from pipeline_v2 import get_dataset, get_dataset_from_txt_files\n",
    "from losses import total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b58649-c3a9-4038-aae0-305aaaf49c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffcc72-77fc-4ba9-828b-06b47c0c009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=[128, 128, 3], name='input_image')\n",
    "    tar_score = tf.keras.layers.Input(shape=[128, 128, 1], name='target_score_map')\n",
    "    tar_geo = tf.keras.layers.Input(shape=[128, 128, 5], name='target_geo_map')\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar_score, tar_geo])  \n",
    "\n",
    "    down1 = downsample(64, 4, False)(x) \n",
    "    down2 = downsample(128, 4)(down1) \n",
    "    down3 = downsample(256, 4)(down2)  \n",
    "    down4 = downsample(512, 4)(down3) \n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down4)  \n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1) \n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) \n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar_score, tar_geo], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009029ad-93b1-49f3-9488-747fd5012bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 100\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(disc_generated_output, score_map, gen_score_map, geo_map, \n",
    "                   gen_geo_map, training_mask):\n",
    "    \n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    loss = total_loss(score_map, gen_score_map, geo_map, gen_geo_map, training_mask)\n",
    "\n",
    "    total_gen_loss = gan_loss + LAMBDA * loss\n",
    "\n",
    "    return total_gen_loss, gan_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc3752-b1d3-4ebe-913a-53fbbc17526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9450000-3f66-4e29-a6be-12945a334d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D0006-0285025.tiff\n",
      "initial_image_shape:  (3800, 4550, 3)\n",
      "New image shape:  (3808, 4576, 3)\n",
      "([0], [3808])\n",
      "([0, 4096], [4096, 480])\n",
      "predicting tile 1 of 2\n",
      "tile_shape (1, 3808, 4096, 3)\n",
      "predicting tile 2 of 2\n",
      "tile_shape (1, 3808, 480, 3)\n",
      "Number of initially detected boxes:  33482\n",
      "LANMS...\n"
     ]
    }
   ],
   "source": [
    "from predict_w_loc_args import predict_v2\n",
    "\n",
    "valid_impath = '/media/gerasimos/Νέος τόμος/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/general_dataset/images/D0006-0285025.tiff'\n",
    "print(os.path.basename(valid_impath))\n",
    "d = predict_v2(generator, valid_impath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdfcc0de-43cb-4d74-9c7b-15937f22f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_tools import parse_boxes_from_json\n",
    "\n",
    "jsonpath = '/media/gerasimos/Νέος τόμος/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/general_dataset/json/D0006-0285025.json'\n",
    "\n",
    "[_,gt_polys,gt_labels] = parse_boxes_from_json(jsonpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ada9085-2f0c-4859-81b8-35731e720849",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [1000] * len(gt_polys)\n",
    "gt_labels = (0, ) * len(gt_labels)\n",
    "gt = {\n",
    "    'polygons' : gt_polys,\n",
    "    'labels' : gt_labels,\n",
    "    'scores': scores\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c893b5bd-5f3f-4efa-a39e-7e270bcabebc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_stats,total_stats \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_predictions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmatch_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43miou_match_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/gerasimos/Νέος τόμος/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/stats.py:317\u001b[0m, in \u001b[0;36mevaluate_predictions\u001b[0;34m(ground_truths, predictions, match_labels, iou_match_thresh, dont_care_overlap_thresh)\u001b[0m\n\u001b[1;32m    314\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m ground_truths[sample]\n\u001b[1;32m    315\u001b[0m prediction \u001b[38;5;241m=\u001b[39m predictions[sample]\n\u001b[0;32m--> 317\u001b[0m [results,sample_stats] \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m stats[sample] \u001b[38;5;241m=\u001b[39m sample_stats\n\u001b[1;32m    321\u001b[0m total_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/media/gerasimos/Νέος τόμος/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/stats.py:270\u001b[0m, in \u001b[0;36mevaluate_predictions.<locals>.process_sample\u001b[0;34m(ground_truth, prediction)\u001b[0m\n\u001b[1;32m    267\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    268\u001b[0m matches \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# Booleans indicating whether the detection was correct\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m gt_dont_cares  \u001b[38;5;241m=\u001b[39m find_dont_cares( \u001b[43mground_truth\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m )\n\u001b[1;32m    271\u001b[0m det_dont_cares \u001b[38;5;241m=\u001b[39m match_dont_cares( gt_dont_cares, ground_truth[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolygons\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    272\u001b[0m                                    prediction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolygons\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    273\u001b[0m matched \u001b[38;5;241m=\u001b[39m get_matches( ground_truth[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolygons\u001b[39m\u001b[38;5;124m'\u001b[39m], prediction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolygons\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    274\u001b[0m                        gt_dont_cares, det_dont_cares )\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "sample_stats,total_stats = stats.evaluate_predictions(\n",
    "      gt,\n",
    "      d,\n",
    "      match_labels=False,\n",
    "      iou_match_thresh=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec5f68-0c6f-42bf-bbbd-decc14e9dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1decf-5d7e-43d3-9652-5933afe9cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in d['polygons']:\n",
    "    print(line[0, 1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69ca27-b276-40c4-bb48-ff2e9b3f7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbdffd11-651c-41e3-8bfd-5718a9d4ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_score_map(tile, gt_score_map, pred_score_map, threshold=None):\n",
    "    pred_score_map = np.where(pred_score_map > threshold, 1, 0)\n",
    "    COLORMAP = 'gray'\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
    "    ax[0].imshow(np.squeeze(tf.cast(tile, tf.uint8)))\n",
    "    ax[1].imshow(np.squeeze(gt_score_map), cmap=COLORMAP)\n",
    "    ax[2].imshow(np.squeeze(pred_score_map), cmap=COLORMAP)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e9de2ba-a9a0-44ee-8b76-97256a03564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepDecayLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, decay_rate, decay_every_n_steps):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_every_n_steps = decay_every_n_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        exp = tf.floor(step/self.decay_every_n_steps)\n",
    "        return self.initial_learning_rate * tf.pow(self.decay_rate, exp)\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, decay_rate, decay_on_step):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_on_step = decay_on_step\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        new_lr = tf.cond(step < self.decay_on_step, \n",
    "                       lambda: self.initial_learning_rate,\n",
    "                       lambda: self.initial_learning_rate * self.decay_rate)\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1563854-dc2c-46f0-ae61-eceef6837cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/gerasimos/Νέος τόμος/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/ckpts/models/0\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = 'D:/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/general_dataset/'\n",
    "INITIAL_DIR = 'D:/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/ckpts/models'\n",
    "\n",
    "conf_file_dir = askdirectory(initialdir=INITIAL_DIR, title='Select the directory '\n",
    "    'with the configuration files')\n",
    "print(conf_file_dir)\n",
    "\n",
    "# Create log files\n",
    "log_dir = os.path.join(conf_file_dir, 'gan_logs')\n",
    "sum_writer = tf.summary.create_file_writer(os.path.join(\n",
    "    log_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d977dde8-a6f3-4266-8304-a5f9d0f7d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "generator = maptd_model()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Define the optimizers\n",
    "step_to_reduce_lr=2**17\n",
    "initial_lr = 1e-4\n",
    "decay_rate = 0.1\n",
    "generator_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=StepDecayLRSchedule(initial_lr, \n",
    "                                      decay_rate,\n",
    "                                      step_to_reduce_lr), epsilon=1e-8)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "\n",
    "# Initialize checkpoints\n",
    "ckpt_dir = os.path.join(conf_file_dir, 'gan_ckpts')\n",
    "ckpt_prefix = os.path.join(ckpt_dir, 'ckpt')\n",
    "ckpt = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                           discriminator_optimizer=discriminator_optimizer,\n",
    "                           generator=generator,\n",
    "                           discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e516745-a2d3-4a2d-8c50-87316cff2664",
   "metadata": {},
   "source": [
    "#### Run the commands in the following cell if you want to continue from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c6ce21e-33f6-4f7b-85a9-edf870f75f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/gerasimos/Νέος τόμος/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/ckpts/models/0/gan_ckpts/ckpt-39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f44456c1580>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################\n",
    "ckpt = tf.train.Checkpoint(generator=generator)\n",
    "###############################NOTE: Check this!\n",
    "\n",
    "\n",
    "latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "print(latest)\n",
    "ckpt.restore(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd3f3f-16f5-48e9-890b-6c945ee607fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(generator, discriminator, tile, \n",
    "               score_map, geo_map, training_mask, \n",
    "               step, summary_writer):\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        \n",
    "        gen_score_map, gen_geo_map = generator(tile, training=True)\n",
    "        \n",
    "        resized_input_image = tile[:, ::4, ::4, :]\n",
    "        \n",
    "        disc_real_output = discriminator([resized_input_image, score_map, \n",
    "                                          geo_map], training=True)\n",
    "        \n",
    "        disc_generated_output = discriminator([resized_input_image, \n",
    "                                               gen_score_map, gen_geo_map], \n",
    "                                              training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_loss = generator_loss(\n",
    "            disc_generated_output, score_map, gen_score_map, geo_map, \n",
    "            gen_geo_map, training_mask)\n",
    "        \n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_loss', gen_loss, step=step//1000)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b442fbc-7259-46a3-b4af-24a19f9572da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(generator, discriminator, train_dataset, test_dataset, summary_writer,\n",
    "        total_steps=2**20, enumerate_from=0):\n",
    "   \n",
    "    training_start = time.time()\n",
    "    start = training_start\n",
    "    for step, (tile, score_map, geo_map, training_mask) in \\\n",
    "                train_dataset.repeat().take(total_steps - enumerate_from).\\\n",
    "                enumerate(start=enumerate_from):\n",
    "        \n",
    "        train_step(generator, discriminator, tile,\n",
    "               score_map, geo_map, training_mask, \n",
    "               step, summary_writer)\n",
    "        \n",
    "        step = step.numpy()\n",
    "        \n",
    "        if (step + 1) % 500 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            print(datetime.datetime.now().strftime(\"%H:%M:%S\"))            \n",
    "            print(f'Step {step + 1}/{total_steps}')\n",
    "            \n",
    "            if step != 0 and step != enumerate_from:\n",
    "                print(f'Time taken for the last 500 steps: '\n",
    "                      f'{time.time()-start:.2f} sec')\n",
    "                estimated_remaining_time = int((time.time() - training_start) / \\\n",
    "                    (step - enumerate_from) * (total_steps - step))\n",
    "                print(f'Estimated time for the training to finish: '\n",
    "                      f'{estimated_remaining_time // 3600} hrs, '\n",
    "                      f'{int(estimated_remaining_time % 3600 / 60)} mins')\n",
    "            \n",
    "            print('Current Learning Rate: ', generator_optimizer.lr(step))\n",
    "            \n",
    "            example_tile, example_score_map, _, _ = next(iter(test_dataset.take(1)))\n",
    "            example_pred_score_map, _ = generator(example_tile, training=True)\n",
    "            show_score_map(example_tile, \n",
    "                           example_score_map, \n",
    "                           example_pred_score_map, \n",
    "                           threshold=0.8)    \n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "        if (step + 1) % 10000 == 0:\n",
    "            ckpt.save(file_prefix=ckpt_prefix)\n",
    "            print(f'Checkpoint at step: {step + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f1ab9-3241-4077-aa41-b3fbd5e05314",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 8\n",
    "# Load the dataset generators\n",
    "train_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'train', \n",
    "                                      batch_size=TRAIN_BATCH_SIZE)\n",
    "test_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'test', \n",
    "                                     batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811dd45-1e7d-4b29-aa56-4752e9125358",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(generator, discriminator, train_ds, test_ds, sum_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086cea36-c83a-491b-87f7-feedd9a52944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
