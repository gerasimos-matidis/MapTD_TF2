{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41561d62-704b-4ec9-b2b7-439a01cfb793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 15:34:47.260631: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 15:34:47.928654: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/gerasimos/miniconda3/envs/dlenv/lib/\n",
      "2022-12-12 15:34:47.928690: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/gerasimos/miniconda3/envs/dlenv/lib/\n",
      "2022-12-12 15:34:47.928695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from maptd_model import maptd_model\n",
    "from IPython import display\n",
    "import os\n",
    "from shutil import rmtree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from tkinter.filedialog import askdirectory\n",
    "\n",
    "from pipeline_v2 import get_dataset, get_dataset_from_txt_files\n",
    "from losses import total_loss\n",
    "import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b58649-c3a9-4038-aae0-305aaaf49c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ffcc72-77fc-4ba9-828b-06b47c0c009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=[128, 128, 3], name='input_image')\n",
    "    tar_score = tf.keras.layers.Input(shape=[128, 128, 1], name='target_score_map')\n",
    "    tar_geo = tf.keras.layers.Input(shape=[128, 128, 5], name='target_geo_map')\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar_score, tar_geo])  \n",
    "\n",
    "    down1 = downsample(64, 4, False)(x) \n",
    "    down2 = downsample(128, 4)(down1) \n",
    "    down3 = downsample(256, 4)(down2)  \n",
    "    down4 = downsample(512, 4)(down3) \n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down4)  \n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1) \n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) \n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar_score, tar_geo], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009029ad-93b1-49f3-9488-747fd5012bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(disc_generated_output, score_map, gen_score_map, geo_map, \n",
    "                   gen_geo_map, training_mask):\n",
    "    \n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    loss = total_loss(score_map, gen_score_map, geo_map, gen_geo_map, training_mask)\n",
    "\n",
    "    total_gen_loss = gan_loss + LAMBDA * loss\n",
    "\n",
    "    return total_gen_loss, gan_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70fc3752-b1d3-4ebe-913a-53fbbc17526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbdffd11-651c-41e3-8bfd-5718a9d4ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_score_map(tile, gt_score_map, pred_score_map, threshold=None):\n",
    "    pred_score_map = np.where(pred_score_map > threshold, 1, 0)\n",
    "    COLORMAP = 'gray'\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
    "    ax[0].imshow(np.squeeze(tf.cast(tile, tf.uint8)))\n",
    "    ax[1].imshow(np.squeeze(gt_score_map), cmap=COLORMAP)\n",
    "    ax[2].imshow(np.squeeze(pred_score_map), cmap=COLORMAP)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e9de2ba-a9a0-44ee-8b76-97256a03564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepDecayLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, decay_rate, decay_every_n_steps):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_every_n_steps = decay_every_n_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        exp = tf.floor(step/self.decay_every_n_steps)\n",
    "        return self.initial_learning_rate * tf.pow(self.decay_rate, exp)\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, decay_rate, decay_on_step):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_on_step = decay_on_step\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        new_lr = tf.cond(step < self.decay_on_step, \n",
    "                       lambda: self.initial_learning_rate,\n",
    "                       lambda: self.initial_learning_rate * self.decay_rate)\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1563854-dc2c-46f0-ae61-eceef6837cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/gerasimos/Νέος τόμος/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/ckpts/models/0\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = './data/general_dataset/'\n",
    "INITIAL_DIR = './data/ckpts/models'\n",
    "\n",
    "conf_file_dir = askdirectory(initialdir=INITIAL_DIR, title='Select the directory '\n",
    "    'with the configuration files')\n",
    "\n",
    "best_model_dir = os.path.join(conf_file_dir, 'best_gan')\n",
    "print(conf_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ea7765-5c7f-453c-a4d7-eb5b5937cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log files\n",
    "log_dir = os.path.join(conf_file_dir, 'gan_logs')\n",
    "sum_writer = tf.summary.create_file_writer(os.path.join(\n",
    "    log_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d977dde8-a6f3-4266-8304-a5f9d0f7d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "generator = maptd_model()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Define the optimizers\n",
    "step_to_reduce_lr=2**17\n",
    "initial_lr = 1e-4\n",
    "decay_rate = 0.1\n",
    "generator_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=StepDecayLRSchedule(initial_lr, \n",
    "                                      decay_rate,\n",
    "                                      step_to_reduce_lr), epsilon=1e-8)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "\n",
    "# Initialize checkpoints\n",
    "ckpt_dir = os.path.join(conf_file_dir, 'gan_ckpts')\n",
    "ckpt_prefix = os.path.join(ckpt_dir, 'ckpt')\n",
    "ckpt = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                           discriminator_optimizer=discriminator_optimizer,\n",
    "                           generator=generator,\n",
    "                           discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e516745-a2d3-4a2d-8c50-87316cff2664",
   "metadata": {},
   "source": [
    "#### Run the commands in the following cell if you want to continue from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c6ce21e-33f6-4f7b-85a9-edf870f75f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/gerasimos/Νέος τόμος/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/ckpts/models/0/gan_ckpts/ckpt-104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f084c2819a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################\n",
    "ckpt = tf.train.Checkpoint(generator=generator)\n",
    "###############################NOTE: Check this!\n",
    "latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "print(latest)\n",
    "ckpt.restore(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc2f7c-8999-4e77-b71f-bbf59dfff50a",
   "metadata": {},
   "source": [
    "#### VALIDATION PROCESSES (TODO: CHANGE THE TITLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdfcc0de-43cb-4d74-9c7b-15937f22f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_impath = './data/general_dataset/images/D0006-0285025.tiff'\n",
    "jsonpath = './data/general_dataset/json/D0006-0285025.json'\n",
    "\n",
    "def validate(valid_impath, jsonpath):\n",
    "    # NOTE: Currently works for one image\n",
    "    \"\"\"\n",
    "    Calculates the Average Precicion for the validation image\n",
    "    \"\"\"\n",
    "    from predict_w_loc_args import predict_v2\n",
    "    from data_tools import parse_boxes_from_json\n",
    "    \n",
    "    base = os.path.splitext(os.path.basename(valid_impath))[0]\n",
    "    prediction = {}\n",
    "    prediction[base] = predict_v2(generator, valid_impath, \n",
    "                                  tile_shape=(1024, 1024), \n",
    "                                  tile_overlap=512,\n",
    "                                  detect_thresh=0.7)\n",
    "\n",
    "    [_,gt_polys,gt_labels] = parse_boxes_from_json(jsonpath)\n",
    "    ground_truth = {}\n",
    "    ground_truth[base] = {\n",
    "        'polygons' : gt_polys,\n",
    "        'labels': gt_labels\n",
    "    }\n",
    "    \n",
    "    sample_stats, _ = stats.evaluate_predictions(ground_truth, prediction,\n",
    "                                                 match_labels=False,\n",
    "                                                 iou_match_thresh=0.5)\n",
    "    return sample_stats[base]['ap'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6abd3f3f-16f5-48e9-890b-6c945ee607fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(generator, discriminator, tile, \n",
    "               score_map, geo_map, training_mask, \n",
    "               step, summary_writer):\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        \n",
    "        gen_score_map, gen_geo_map = generator(tile, training=True)\n",
    "        \n",
    "        resized_input_image = tile[:, ::4, ::4, :]\n",
    "        \n",
    "        disc_real_output = discriminator([resized_input_image, score_map, \n",
    "                                          geo_map], training=True)\n",
    "        \n",
    "        disc_generated_output = discriminator([resized_input_image, \n",
    "                                               gen_score_map, gen_geo_map], \n",
    "                                              training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_loss = generator_loss(\n",
    "            disc_generated_output, score_map, gen_score_map, geo_map, \n",
    "            gen_geo_map, training_mask)\n",
    "        \n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_loss', gen_loss, step=step//1000)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b442fbc-7259-46a3-b4af-24a19f9572da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(generator, discriminator, train_dataset, test_dataset, summary_writer,\n",
    "        total_steps=2**20, enumerate_from=0):\n",
    "    average_precision = 0\n",
    "    training_start = time.time()\n",
    "    start = training_start\n",
    "    for step, (tile, score_map, geo_map, training_mask) in \\\n",
    "                train_dataset.repeat().take(total_steps - enumerate_from).\\\n",
    "                enumerate(start=enumerate_from):\n",
    "        \n",
    "        train_step(generator, discriminator, tile,\n",
    "               score_map, geo_map, training_mask, \n",
    "               step, summary_writer)\n",
    "        \n",
    "        step = step.numpy()\n",
    "        \n",
    "        if (step + 1) % 500 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            print(datetime.datetime.now().strftime(\"%H:%M:%S\"))            \n",
    "            print(f'Step {step + 1}/{total_steps}')\n",
    "            \n",
    "            if step != 0 and step != enumerate_from:\n",
    "                print(f'Time taken for the last 500 steps: '\n",
    "                      f'{time.time()-start:.2f} sec')\n",
    "                estimated_remaining_time = int((time.time() - training_start) / \\\n",
    "                    (step - enumerate_from) * (total_steps - step))\n",
    "                print(f'Estimated time for the training to finish: '\n",
    "                      f'{estimated_remaining_time // 3600} hrs, '\n",
    "                      f'{int(estimated_remaining_time % 3600 / 60)} mins')\n",
    "            \n",
    "            #print('Current Learning Rate: ', generator_optimizer.lr(step)) # NOTE: use ths on Windows\n",
    "            print('Current Learning Rate: ', generator_optimizer.lr.numpy()) # NOTE: Use this on Linux\n",
    "            \n",
    "            example_tile, example_score_map, _, _ = next(iter(test_dataset.take(1)))\n",
    "            example_pred_score_map, _ = generator(example_tile, training=True)\n",
    "            show_score_map(example_tile, \n",
    "                           example_score_map, \n",
    "                           example_pred_score_map, \n",
    "                           threshold=0.8)    \n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "        # Save the model if the validation image improves the average precision\n",
    "        # (best model)\n",
    "        if (step + 1) % 100 == 0:\n",
    "            new_average_precision = validate(valid_impath, jsonpath)\n",
    "            if new_average_precision > average_precision:\n",
    "                generator.save(os.path.join(conf_file_dir, 'best_gan'))\n",
    "                print(f'Old AP = {average_precision}, new AP = '\n",
    "                      f'{new_average_precision}. A new best model was saved at '\n",
    "                      f'step{step + 1}.')\n",
    "                \n",
    "                rmtree(best_model_dir)\n",
    "                os.mkdir(best_model_dir)\n",
    "                with open(os.path.join(best_model_dir, 'best_model_step.txt'), \n",
    "                          'w') as f:\n",
    "                    f.write(f'Best model saved at step: {step + 1}')   \n",
    "                \n",
    "                \n",
    "                average_precision = new_average_precision\n",
    "            \n",
    "        if (step + 1) % 10000 == 0:\n",
    "            ckpt.save(file_prefix=ckpt_prefix)\n",
    "            print(f'Checkpoint at step: {step + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "909f1ab9-3241-4077-aa41-b3fbd5e05314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/gerasimos/Νέος τόμος/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/pipeline_v2.py:160: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From /home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:458: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 8\n",
    "# Load the dataset generators\n",
    "train_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'train', \n",
    "                                      batch_size=TRAIN_BATCH_SIZE)\n",
    "test_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'test', \n",
    "                                     batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a811dd45-1e7d-4b29-aa56-4752e9125358",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput' defined at (most recent call last):\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/traitlets/config/application.py\", line 985, in launch_instance\n      app.start()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3138/1157537285.py\", line 1, in <module>\n      fit(generator, discriminator, train_ds, test_ds, sum_writer)\n    File \"/tmp/ipykernel_3138/3652473701.py\", line 10, in fit\n      train_step(generator, discriminator, tile,\n    File \"/tmp/ipykernel_3138/3398258951.py\", line 27, in train_step\n      discriminator_gradients = disc_tape.gradient(disc_loss,\nNode: 'gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput'\nDetected at node 'gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput' defined at (most recent call last):\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/traitlets/config/application.py\", line 985, in launch_instance\n      app.start()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3138/1157537285.py\", line 1, in <module>\n      fit(generator, discriminator, train_ds, test_ds, sum_writer)\n    File \"/tmp/ipykernel_3138/3652473701.py\", line 10, in fit\n      train_step(generator, discriminator, tile,\n    File \"/tmp/ipykernel_3138/3398258951.py\", line 27, in train_step\n      discriminator_gradients = disc_tape.gradient(disc_loss,\nNode: 'gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput'\n2 root error(s) found.\n  (0) NOT_FOUND:  No algorithm worked!  Error messages:\n  Profiling failure on CUDNN engine 1#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 30539792 bytes.\n  Profiling failure on CUDNN engine 1: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 3#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 199884816 bytes.\n  Profiling failure on CUDNN engine 3: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 199884816 bytes.\n  Profiling failure on CUDNN engine 0#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 0: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n\t [[{{node gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput}}]]\n\t [[add_12/_14]]\n  (1) NOT_FOUND:  No algorithm worked!  Error messages:\n  Profiling failure on CUDNN engine 1#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 30539792 bytes.\n  Profiling failure on CUDNN engine 1: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 3#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 199884816 bytes.\n  Profiling failure on CUDNN engine 3: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 199884816 bytes.\n  Profiling failure on CUDNN engine 0#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 0: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n\t [[{{node gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_step_33125]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msum_writer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(generator, discriminator, train_dataset, test_dataset, summary_writer, total_steps, enumerate_from)\u001b[0m\n\u001b[1;32m      5\u001b[0m start \u001b[38;5;241m=\u001b[39m training_start\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (tile, score_map, geo_map, training_mask) \u001b[38;5;129;01min\u001b[39;00m \\\n\u001b[1;32m      7\u001b[0m             train_dataset\u001b[38;5;241m.\u001b[39mrepeat()\u001b[38;5;241m.\u001b[39mtake(total_steps \u001b[38;5;241m-\u001b[39m enumerate_from)\u001b[38;5;241m.\u001b[39m\\\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;28menumerate\u001b[39m(start\u001b[38;5;241m=\u001b[39menumerate_from):\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m           \u001b[49m\u001b[43mscore_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeo_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     step \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlenv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/dlenv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput' defined at (most recent call last):\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/traitlets/config/application.py\", line 985, in launch_instance\n      app.start()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3138/1157537285.py\", line 1, in <module>\n      fit(generator, discriminator, train_ds, test_ds, sum_writer)\n    File \"/tmp/ipykernel_3138/3652473701.py\", line 10, in fit\n      train_step(generator, discriminator, tile,\n    File \"/tmp/ipykernel_3138/3398258951.py\", line 27, in train_step\n      discriminator_gradients = disc_tape.gradient(disc_loss,\nNode: 'gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput'\nDetected at node 'gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput' defined at (most recent call last):\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/traitlets/config/application.py\", line 985, in launch_instance\n      app.start()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/gerasimos/miniconda3/envs/dlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3138/1157537285.py\", line 1, in <module>\n      fit(generator, discriminator, train_ds, test_ds, sum_writer)\n    File \"/tmp/ipykernel_3138/3652473701.py\", line 10, in fit\n      train_step(generator, discriminator, tile,\n    File \"/tmp/ipykernel_3138/3398258951.py\", line 27, in train_step\n      discriminator_gradients = disc_tape.gradient(disc_loss,\nNode: 'gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput'\n2 root error(s) found.\n  (0) NOT_FOUND:  No algorithm worked!  Error messages:\n  Profiling failure on CUDNN engine 1#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 30539792 bytes.\n  Profiling failure on CUDNN engine 1: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 3#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 199884816 bytes.\n  Profiling failure on CUDNN engine 3: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 199884816 bytes.\n  Profiling failure on CUDNN engine 0#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 0: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n\t [[{{node gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput}}]]\n\t [[add_12/_14]]\n  (1) NOT_FOUND:  No algorithm worked!  Error messages:\n  Profiling failure on CUDNN engine 1#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 30539792 bytes.\n  Profiling failure on CUDNN engine 1: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 3#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 199884816 bytes.\n  Profiling failure on CUDNN engine 3: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 199884816 bytes.\n  Profiling failure on CUDNN engine 0#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 0: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n\t [[{{node gradient_tape/model_1/sequential_1/conv2d_8/Conv2D/Conv2DBackpropInput}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_step_33125]"
     ]
    }
   ],
   "source": [
    "fit(generator, discriminator, train_ds, test_ds, sum_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a2870-3797-4257-b0bb-57f2acc7bf92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1d8acd67561793d0d4711492c21aa45930fc46ff826a8c13f35cfa9289d69f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
