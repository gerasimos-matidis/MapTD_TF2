{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b952e7-3166-4908-a0b7-bb8c68e6250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Disables INFO & WARNING logs\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from IPython import display\n",
    "from tkinter.filedialog import askdirectory\n",
    "\n",
    "from losses import total_loss\n",
    "from maptd_model import maptd_model\n",
    "from pipeline_v2 import get_dataset, get_dataset_from_txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf33aae-738d-4b1b-a0e9-963fb9c6ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './data/general_dataset/'\n",
    "INITIAL_DIR = './data/ckpts/models'\n",
    "\n",
    "conf_file_dir = askdirectory(initialdir=INITIAL_DIR, title='Select the directory '\n",
    "    'with the configuration files')\n",
    "print(conf_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd091e9-55a2-4917-a632-f34c7efd6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_score_map(tile, gt_score_map, pred_score_map, threshold=None):\n",
    "    pred_score_map = np.where(pred_score_map > threshold, 1, 0)\n",
    "    COLORMAP = 'gray'\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
    "    ax[0].imshow(np.squeeze(tf.cast(tile, tf.uint8)))\n",
    "    ax[1].imshow(np.squeeze(gt_score_map), cmap=COLORMAP)\n",
    "    ax[2].imshow(np.squeeze(pred_score_map), cmap=COLORMAP)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de219e-ef12-4d6b-9aa6-d242d714cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepDecayLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, initial_learning_rate, decay_rate, decay_on_step):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_on_step = decay_on_step\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        new_lr = tf.cond(step < self.decay_on_step, \n",
    "                       lambda: self.initial_learning_rate,\n",
    "                       lambda: self.initial_learning_rate * self.decay_rate)\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb28ef4-28de-4af3-bf7b-758ac4b627e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "\n",
    "# Define the model\n",
    "maptd = maptd_model()\n",
    "\n",
    "# Create log files\n",
    "log_dir = os.path.join(conf_file_dir, 'logs')\n",
    "sum_writer = tf.summary.create_file_writer(os.path.join(log_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "# Initialize checkpoints\n",
    "ckpt_dir = os.path.join(conf_file_dir, 'training_ckpts')\n",
    "ckpt_prefix = os.path.join(ckpt_dir, 'ckpt')\n",
    "ckpt = tf.train.Checkpoint(model=maptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f954ea5-5dae-4cc5-a887-d8e70c18c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset generators\n",
    "train_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'train', batch_size=TRAIN_BATCH_SIZE)\n",
    "test_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'test', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800d1f3-2104-41e0-87ce-73f85134e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS TO RESTART THE TRAINING FROM THE LATEST CHECKPOINT\n",
    "latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "print(latest)\n",
    "ckpt.restore(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb730e-70a9-430a-b208-9563c265a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(model, optimizer, tile, \n",
    "               score_map, geo_map, training_mask, \n",
    "               step, summary_writer):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_score_map, pred_geo_map = model(tile, training=True)\n",
    "        loss = total_loss(score_map, pred_score_map, geo_map, pred_geo_map, \n",
    "                          training_mask)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('total_loss', loss, step=step)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f9e95-dc02-4585-8d9c-07c5c5f80160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dataset, test_dataset, summary_writer,\n",
    "        total_steps=2**20, step_to_reduce_lr=2**17, enumerate_from=0):\n",
    "    initial_lr = 1e-4\n",
    "    decay_rate = 0.1\n",
    "    opt = tf.keras.optimizers.Adam(\n",
    "        learning_rate=StepDecayLRSchedule(initial_lr, decay_rate, \n",
    "                                          step_to_reduce_lr), \n",
    "        epsilon=1e-8)\n",
    "    \n",
    "    training_start = time.time()\n",
    "    start = training_start\n",
    "    for step, (tile, score_map, geo_map, training_mask) in \\\n",
    "                train_dataset.repeat().take(total_steps - enumerate_from).\\\n",
    "                enumerate(start=enumerate_from):\n",
    "        \n",
    "        training_loss = train_step(model, opt, tile, \n",
    "                                    score_map, geo_map, training_mask,\n",
    "                                    step, summary_writer)\n",
    "        \n",
    "        step = step.numpy()\n",
    "        \n",
    "        if (step + 1) % 500 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            print(datetime.datetime.now().strftime(\"%H:%M:%S\"))            \n",
    "            print(f'Step {step + 1}/{total_steps}')\n",
    "            \n",
    "            if step != 0 and step != enumerate_from:\n",
    "                print(f'Time taken for the last 500 steps: '\n",
    "                      f'{time.time()-start:.2f} sec')\n",
    "                estimated_remaining_time = int((time.time() - training_start) / \\\n",
    "                    (step - enumerate_from) * (total_steps - step))\n",
    "                print(f'Estimated time for the training to finish: '\n",
    "                      f'{estimated_remaining_time // 3600} hrs, '\n",
    "                      f'{int(estimated_remaining_time % 3600 / 60)} mins')\n",
    "                \n",
    "            print('Current Learning Rate: ', opt.lr(step))\n",
    "            print(f'Training loss: {training_loss:.4f}')\n",
    "            \n",
    "            example_tile, example_score_map, _, _ = next(iter(test_dataset.take(1)))\n",
    "            example_pred_score_map, _ = maptd(example_tile, training=True)\n",
    "            show_score_map(example_tile, \n",
    "                           example_score_map, \n",
    "                           example_pred_score_map, \n",
    "                           threshold=0.9)    \n",
    "            \n",
    "            start = time.time()\n",
    "                       if (step + 1) == step_to_reduce_lr:\n",
    "\n",
    "            new_lr = opt.lr(step)\n",
    "            print(f'\\nThe learning rate for the optimizer was decreased from '\n",
    "                  f'{initial_lr} to {new_lr}')\n",
    "            \n",
    "        if (step + 1) % 10000 == 0:\n",
    "            ckpt.save(file_prefix=ckpt_prefix)\n",
    "            print(f'Checkpoint at step: {step + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab91fd-00bc-4a95-826a-1073ba49e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION!!! IF YOU CONTINUE THE TRAINING FROM A CHECKPOINT YOU MUST SPECIFY THE OPTIONAL ARGUMENT \"enumerate from\" IN THE FOLLOWING COMMAND. \n",
    "# THE VALUE OF THE NUMBER YOU MUST PUT DEPENDS ON THE NUMBER OF THE LATEST CHECKPOINT. FOR EXAMPLE, IF YOU RESTORE FROM ckpt-47, YOU MUST \n",
    "# PUT \"enumerate_from=470000\" (SINCE WE SAVE A CHECKPOINT FOR EVERY 10000 STEPS)\n",
    "\n",
    "# Train the model\n",
<<<<<<< HEAD
    "fit(maptd, train_ds, test_ds, sum_writer, enumerate_from=680000)"
=======
    "fit(maptd, train_ds, test_ds, sum_writer, enumerate_from=340000)"
>>>>>>> a608070d159f3ed274ee8619579bc7c1288a72f7
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9258c-8025-4edb-9426-35347a4932a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = os.path.join(conf_file_dir, 'saved_model')\n",
    "maptd.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "5f896cf9-bf26-4bb1-a075-08ce18ebb314",
   "metadata": {},
   "outputs": [],
   "source": []
=======
   "id": "6ba65b17-e959-42e0-b0f4-a4e574ab5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "    `"
   ]
>>>>>>> a608070d159f3ed274ee8619579bc7c1288a72f7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
