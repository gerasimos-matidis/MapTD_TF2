{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b952e7-3166-4908-a0b7-bb8c68e6250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Disables INFO & WARNING logs\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from IPython import display\n",
    "from tkinter.filedialog import askdirectory\n",
    "\n",
    "from losses import total_loss\n",
    "from maptd_model import maptd_model\n",
    "from pipeline_v2 import get_dataset, get_dataset_from_txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf33aae-738d-4b1b-a0e9-963fb9c6ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'D:/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/general_dataset/'\n",
    "INITIAL_DIR = 'D:/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/ckpts/models'\n",
    "\n",
    "conf_file_dir = askdirectory(initialdir=INITIAL_DIR, title='Select the directory '\n",
    "    'with the configuration files')\n",
    "print(conf_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd091e9-55a2-4917-a632-f34c7efd6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_score_map(tile, gt_score_map, pred_score_map, threshold=None):\n",
    "    pred_score_map = np.where(pred_score_map > threshold, 1, 0)\n",
    "    COLORMAP = 'gray'\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
    "    ax[0].imshow(np.squeeze(tf.cast(tile, tf.uint8)))\n",
    "    ax[1].imshow(np.squeeze(gt_score_map), cmap=COLORMAP)\n",
    "    ax[2].imshow(np.squeeze(pred_score_map), cmap=COLORMAP)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de219e-ef12-4d6b-9aa6-d242d714cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepDecayLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, decay_rate, decay_every_n_steps):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_every_n_steps = decay_every_n_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        exp = tf.floor(step/self.decay_every_n_steps)\n",
    "        return self.initial_learning_rate * tf.pow(self.decay_rate, exp)\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, decay_rate, decay_on_step):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_on_step = decay_on_step\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        new_lr = tf.cond(step < self.decay_on_step, \n",
    "                       lambda: self.initial_learning_rate,\n",
    "                       lambda: self.initial_learning_rate * self.decay_rate)\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375fefe3-ad0f-4f69-89a2-4b415ef4310c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb730e-70a9-430a-b208-9563c265a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(model, optimizer, tile, \n",
    "               score_map, geo_map, training_mask, \n",
    "               step, summary_writer):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_score_map, pred_geo_map = model(tile, training=True)\n",
    "\n",
    "        loss = total_loss(score_map, pred_score_map, geo_map, pred_geo_map, \n",
    "                          training_mask)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('total_loss', loss, step=step)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f9e95-dc02-4585-8d9c-07c5c5f80160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dataset, test_dataset, summary_writer,\n",
    "        total_steps=2**20, step_to_reduce_lr=2**17, enumerate_from=0):\n",
    "    initial_lr = 1e-4\n",
    "    decay_rate = 0.1\n",
    "    opt = tf.keras.optimizers.Adam(\n",
    "        learning_rate=StepDecayLRSchedule(initial_lr, decay_rate, \n",
    "                                          step_to_reduce_lr), \n",
    "        epsilon=1e-8)\n",
    "    \n",
    "    training_start = time.time()\n",
    "    start = training_start\n",
    "    for step, (tile, score_map, geo_map, training_mask) in \\\n",
    "                train_dataset.repeat().take(total_steps - enumerate_from).\\\n",
    "                enumerate(start=enumerate_from):\n",
    "        \n",
    "        training_loss = train_step(model, opt, tile, \n",
    "                                    score_map, geo_map, training_mask,\n",
    "                                    step, summary_writer)\n",
    "        \n",
    "        step = step.numpy()\n",
    "        \n",
    "        if (step + 1) % 500 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            print(datetime.datetime.now().strftime(\"%H:%M:%S\"))            \n",
    "            print(f'Step {step + 1}/{total_steps}')\n",
    "            \n",
    "            if step != 0 and step != enumerate_from:\n",
    "                print(f'Time taken for the last 500 steps: '\n",
    "                      f'{time.time()-start:.2f} sec')\n",
    "                estimated_remaining_time = int((time.time() - training_start) / \\\n",
    "                    (step - enumerate_from) * (total_steps - step))\n",
    "                print(f'Estimated time for the training to finish: '\n",
    "                      f'{estimated_remaining_time // 3600} hrs, '\n",
    "                      f'{int(estimated_remaining_time % 3600 / 60)} mins')\n",
    "                \n",
    "            print('Current Learning Rate: ', opt.lr(step))\n",
    "            print(f'Training loss: {training_loss:.4f}')\n",
    "            \n",
    "            example_tile, example_score_map, _, _ = next(iter(test_dataset.take(1)))\n",
    "            example_pred_score_map, _ = maptd(example_tile, training=True)\n",
    "            show_score_map(example_tile, \n",
    "                           example_score_map, \n",
    "                           example_pred_score_map, \n",
    "                           threshold=0.7)    \n",
    "            \n",
    "            start = time.time()\n",
    "               \n",
    "        if (step + 1) == step_to_reduce_lr:\n",
    "            new_lr = opt.lr(step)\n",
    "            print(f'\\nThe learning rate for the optimizer was decreased from '\n",
    "                  f'{initial_lr} to {new_lr}')\n",
    "            \n",
    "        if (step + 1) % 10000 == 0:\n",
    "            ckpt.save(file_prefix=ckpt_prefix)\n",
    "            print(f'Checkpoint at step: {step + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb28ef4-28de-4af3-bf7b-758ac4b627e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 512\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "\n",
    "# Define the model\n",
    "maptd = maptd_model(input_size=TILE_SIZE)\n",
    "\n",
    "# Create log files\n",
    "log_dir = os.path.join(conf_file_dir, 'logs')\n",
    "sum_writer = tf.summary.create_file_writer(os.path.join(log_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "# Initialize checkpoints\n",
    "ckpt_dir = os.path.join(conf_file_dir, 'training_ckpts')\n",
    "ckpt_prefix = os.path.join(ckpt_dir, 'ckpt')\n",
    "ckpt = tf.train.Checkpoint(model=maptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f954ea5-5dae-4cc5-a887-d8e70c18c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset generators\n",
    "train_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'train', batch_size=TRAIN_BATCH_SIZE)\n",
    "test_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'test', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab91fd-00bc-4a95-826a-1073ba49e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit(maptd, train_ds, test_ds, sum_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9258c-8025-4edb-9426-35347a4932a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = os.path.join(conf_file_dir, 'saved_model')\n",
    "maptd_model.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33578c10-de65-49fa-aefd-202ad852d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'D:/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/general_dataset/'\n",
    "test_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'test', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aad5a5-7a5d-4d9a-b4ee-0006318b5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tile, example_score_map, _, _ = next(iter(test_ds.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70048d5f-3782-4d22-bd59-ad52bafba1e5",
   "metadata": {},
   "source": [
    "## Restore a model from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570323f-4d0f-4ce0-9666-2a13b4e9c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 512\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "\n",
    "# Load the dataset generators\n",
    "train_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'train', batch_size=TRAIN_BATCH_SIZE)\n",
    "test_ds = get_dataset_from_txt_files(dataset_dir, conf_file_dir, 'test', batch_size=1)\n",
    "\n",
    "# Define the model\n",
    "maptd = maptd_model(input_size=TILE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211e2d5-43a0-4071-bb7e-f9d8e14333e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpts_dir = askdirectory(initialdir=INITIAL_DIR, \n",
    "                         title='Select the directory with the checkpoints')\n",
    "\n",
    "ckpt_prefix = os.path.join(ckpts_dir, 'ckpt')\n",
    "ckpt = tf.train.Checkpoint(model=maptd)\n",
    "latest = tf.train.latest_checkpoint(ckpts_dir)\n",
    "print(latest)\n",
    "ckpt.restore(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b2749-c9dd-41cf-a05d-a6763ca52c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_file_dir = 'D:/Gerasimos/Toponym_Recognition/MapTD_General/MapTD_TF2/data/ckpts/models/0'\n",
    "log_dir = os.path.join(conf_file_dir, 'logs')\n",
    "sum_writer = tf.summary.create_file_writer(os.path.join(log_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117a6e5-d74e-4ebe-9f47-10b3fbcabc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepnumber_to_start_from = 490000 # Because, this time, the latest ckpt is the one with number 15, i.e. 15*10000=150000 steps have ran already\n",
    "\n",
    "# Continue on train the model\n",
    "fit(maptd, train_ds, test_ds, sum_writer, enumerate_from=stepnumber_to_start_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8e427-8082-4112-9a74-205d445d8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = os.path.join(conf_file_dir, 'saved_model')\n",
    "maptd_model.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4d5a3-6585-4a6d-a0dd-1e7059ee4853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepenv",
   "language": "python",
   "name": "deepenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
